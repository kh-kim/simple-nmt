import argparse
import pprint

import torch
from torch import optim
import torch.nn as nn

from simple_nmt.dataset import SPECIAL_TOKENS
from simple_nmt.dataset import (
    read_text,
    get_vocab,
    MachineTranslationDataset,
    MachineTranslationCollator,
)

from simple_nmt.models.rnnlm import LanguageModel
from simple_nmt.lm_trainer import LanguageModelTrainer as LMTrainer

from train import get_loaders
from dual_train import get_crits


def define_argparser(is_continue=False):
    p = argparse.ArgumentParser()

    if is_continue:
        p.add_argument(
            '--load_fn',
            required=True,
            help='Model file name to continue.'
        )

    p.add_argument(
        '--model_fn',
        required=not is_continue,
        help='Model file name to save. Additional information would be annotated to the file name.'
    )
    p.add_argument(
        '--train',
        required=not is_continue,
        help='Training set file name except the extention. (ex: train.en --> train)'
    )
    p.add_argument(
        '--valid',
        required=not is_continue,
        help='Validation set file name except the extention. (ex: valid.en --> valid)'
    )
    p.add_argument(
        '--lang',
        required=not is_continue,
        help='Set of extention represents language pair. (ex: en + ko --> enko)'
    )
    p.add_argument(
        '--gpu_id',
        type=int,
        default=-1,
        help='GPU ID to train. Currently, GPU parallel is not supported. -1 for CPU. Default=%(default)s'
    )
    p.add_argument(
        '--off_autocast',
        action='store_true',
        help='Turn-off Automatic Mixed Precision (AMP), which speed-up training.',
    )

    p.add_argument(
        '--batch_size',
        type=int,
        default=32,
        help='Mini batch size for gradient descent. Default=%(default)s'
    )
    p.add_argument(
        '--n_epochs',
        type=int,
        default=20,
        help='Number of epochs to train. Default=%(default)s'
    )
    p.add_argument(
        '--verbose',
        type=int,
        default=2,
        help='VERBOSE_SILENT, VERBOSE_EPOCH_WISE, VERBOSE_BATCH_WISE = 0, 1, 2. Default=%(default)s'
    )

    p.add_argument(
        '--max_length',
        type=int,
        default=100,
        help='Maximum length of the training sequence. Default=%(default)s'
    )
    p.add_argument(
        '--dropout',
        type=float,
        default=.2,
        help='Dropout rate. Default=%(default)s'
    )
    p.add_argument(
        '--word_vec_size',
        type=int,
        default=512,
        help='Word embedding vector dimension. Default=%(default)s'
    )
    p.add_argument(
        '--hidden_size',
        type=int,
        default=768,
        help='Hidden size of LSTM. Default=%(default)s'
    )
    p.add_argument(
        '--n_layers',
        type=int,
        default=4,
        help='Number of layers in LSTM. Default=%(default)s'
    )
    p.add_argument(
        '--max_grad_norm',
        type=float,
        default=1e+8,
        help='Threshold for gradient clipping. Default=%(default)s'
    )

    config = p.parse_args()

    return config


def get_models(src_vocab_size, tgt_vocab_size, config):
    language_models = [
        LanguageModel(
            tgt_vocab_size,
            config.word_vec_size,
            config.hidden_size,
            n_layers=config.n_layers,
            dropout_p=config.dropout,
        ),
        LanguageModel(
            src_vocab_size,
            config.word_vec_size,
            config.hidden_size,
            n_layers=config.n_layers,
            dropout_p=config.dropout,
        ),
    ]

    return language_models


def main(config):
    def print_config(config):
        pp = pprint.PrettyPrinter(indent=4)
        pp.pprint(vars(config))
    print_config(config)

    train_loader, valid_loader, src_vocab, tgt_vocab = get_loaders(
        config,
        is_dsl=False,
    )

    src_vocab_size = len(src_vocab)
    tgt_vocab_size = len(tgt_vocab)

    models = get_models(
        src_vocab_size,
        tgt_vocab_size,
        config
    )

    crits = get_crits(
        src_vocab_size,
        tgt_vocab_size,
        pad_index=SPECIAL_TOKENS.PAD_idx
    )

    if config.gpu_id >= 0:
        for model, crit in zip(models, crits):
            model.cuda(config.gpu_id)
            crit.cuda(config.gpu_id)

    if config.verbose >= 2:
        print(models)

    for model, crit in zip(models, crits):
        optimizer = optim.Adam(model.parameters())
        lm_trainer = LMTrainer(config)

        model = lm_trainer.train(
            model, crit, optimizer,
            train_loader=train_loader,
            valid_loader=valid_loader,
            src_vocab=src_vocab if model.vocab_size == src_vocab_size else None,
            tgt_vocab=tgt_vocab if model.vocab_size == tgt_vocab_size else None,
            n_epochs=config.n_epochs,
        )

    torch.save(
        {
            'model': [
                models[0].state_dict(),
                models[1].state_dict(),
            ],
            'config': config,
            'src_vocab': src_vocab,
            'tgt_vocab': tgt_vocab,
        }, config.model_fn
    )


if __name__ == '__main__':
    config = define_argparser()
    main(config)
